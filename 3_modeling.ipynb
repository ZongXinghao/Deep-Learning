{"cells":[{"cell_type":"markdown","source":["#### Imports ans installations"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4fcdee7a-eab0-4bb8-9e7c-5ab483f4c0fc"}}},{"cell_type":"code","source":["%sh pip install keras"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ea4e85be-aefd-4864-b194-af760ce13025"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sh pip install handyspark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"09d785e5-33c9-4c97-be77-4416de808cef"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sh pip install mlflow"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3590c5a-5b33-4584-a4b6-37e65fa767a9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sh pip install tensorflow"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc632506-2586-4980-a66b-760b2e26a482"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\nimport pyspark.sql.types as T\nimport time\n#from tensorflow.keras.layers import Dense, Flatten, Conv2D\n#from tensorflow.keras import Model\nimport os\nimport tempfile\nimport keras\nimport numpy as np\nimport random\nimport pandas as pd\nfrom pyspark.sql.functions import trim\npd.set_option('max_colwidth', -1) # to prevent truncating of columns in jupyter\n#from six.moves import urllib\n#from pyspark.ml.functions import vector_to_array\n# import libraries\nimport os\nfrom sklearn.preprocessing import MinMaxScaler\n#from sklearn.externals import joblib\nimport seaborn as sns\nsns.set(color_codes=True)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom numpy.random import seed\nimport tensorflow as tf\n#tf.logging.set_verbosity(tf.logging.ERROR)\nfrom multiprocessing.pool import ThreadPool\n\nfrom keras.layers import Input, Dropout, Dense, LSTM, TimeDistributed, RepeatVector\nfrom keras.models import Model\nfrom keras import regularizers\n# set random seed\nseed(10)\nfrom keras.layers import Bidirectional, Concatenate, Permute, Dot, Multiply\nfrom keras.layers import RepeatVector, Dense, Activation, Lambda\nfrom keras.optimizers import Adam\nfrom keras.utils import to_categorical\nfrom keras.models import load_model, Model\nimport keras.backend as K\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom pyspark.ml.pipeline import PipelineModel\nimport pyspark.sql.functions as F\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.functions import rand\nfrom pyspark.sql.types import FloatType\nimport mlflow\nimport mlflow.mleap\nimport mlflow.spark\nimport time\nfrom pyspark.sql import SparkSession\nfrom handyspark import *\nimport findspark\nfrom datetime import datetime, timedelta\nimport mleap.pyspark\nfrom mleap.pyspark.spark_support import SimpleSparkSerializer\nimport random\nimport string\nfrom functools import reduce\nfrom pyspark.sql import DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b954843-4852-4316-96e8-ee5b40878042"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Modeling"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a1e51347-2da5-472a-aba5-66a9fff80406"}}},{"cell_type":"code","source":["def one_step_attention(a, s_prev,repeator, concatenator,densor1, densor2,activator, dotor):\n    \"\"\"\n    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n    \n    Arguments:\n    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n    \n    Returns:\n    context -- context vector, input of the next (post-attention) LSTM cell\n    \"\"\"\n    \n    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\"\n    s_prev = repeator(s_prev)\n    # Use concatenator to concatenate a and s_prev on the last axis \n    concat = concatenator([a,s_prev])\n    # Use densor1 to propagate concat through a small fully-connected neural network to compute the \"intermediate energies\" variable e. \n    e = densor1(concat)\n    # Use densor2 to propagate e through a small fully-connected neural network to compute the \"energies\" variable energies. \n    energies = densor2(e)\n    # Use \"activator\" on \"energies\" to compute the attention weights \"alphas\" \n    alphas = activator(energies)\n    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell.\n    context = dotor([alphas,a])\n    \n    return context\n  \n  \ndef sequence_transcation_fraud_detection_combined_with_entity_embeddings(Tx, Ty, n_a, n_s, input_size, out_size,index_positions_dict,index_original_dimensions_dict, index_desired_dimensions_dict,cat_features_amount):\n    \"\"\"\n    Arguments:\n    Tx -- length of the input sequence\n    Ty -- length of the output sequence\n    n_a -- hidden state size of the Bi-LSTM\n    n_s -- hidden state size of the post-attention LSTM\n\n    Returns:\n    model -- Keras model instance\n    \"\"\"\n    \n    #Define the components of the attention mechanism of which the weights are going to be shared acorss time steps. \n    repeator = RepeatVector(Tx)\n    concatenator = Concatenate(axis=-1)\n    densor1 = Dense(10, activation = \"tanh\")\n    densor2 = Dense(1, activation = \"relu\")\n    activator = Activation(\"softmax\", name='attention_weights') \n    dotor = Dot(axes = 1)\n\n    \n    # Define the post_attention_lstm layer and the dense layers for the final hidden state of the post_attention_lstm layer.\n    post_activation_LSTM_cell = LSTM(n_s, return_state = True) # post-attention LSTM \n    output_layer_1 = Dense(16, activation=\"relu\")\n    output_layer_2 = Dense(16, activation=\"relu\")\n    output_layer_3 = Dense(16, activation=\"relu\")\n    output_layer_4 = Dense(8, activation=\"relu\")\n    output_layer_5 = Dense(1, activation=\"sigmoid\")\n    \n    bn = keras.layers.BatchNormalization()\n    drop = keras.layers.Dropout(0.5)\n    \n    \n    # Define the entity embeddings component for the categorical variables\n    embeddings_transform_layers = {}\n    embeddings_bn_layers = {}\n    embeddings_reshape_layers = {}\n    features_trans_layers = {}\n    features_trans_bn_layers = {}\n    \n            \n    for key in index_positions_dict.keys():\n\n      embeddings_transform_layer_name = key + \"embedding_transform\"\n      embeddings_bn_layer_name = key + \"embedding_bn\"\n      embeddings_reshape_layer_name = key + \"embedding_reshape\"\n      features_trans_layer_name = key + \"features_trans\"\n      features_trans_bn_layer_name = key + \"features_trans_bn\"\n      \n\n      index_original_dimensions= index_original_dimensions_dict[key]\n      index_desired_dimensions = index_desired_dimensions_dict[key]\n\n      #Embeddings\n      embeddings_transform_layer = tf.keras.layers.Embedding(index_original_dimensions,index_desired_dimensions)\n      embeddings_reshape_layer = tf.keras.layers.Reshape((Tx,index_desired_dimensions, ))\n      embeddings_bn_layer = tf.keras.layers.BatchNormalization()\n      \n      #Transformation for the embeddings\n      features_trans_layer = tf.keras.layers.Dense(index_desired_dimensions, activation=\"relu\")\n      features_trans_bn_layer = tf.keras.layers.BatchNormalization()\n      \n      #Add the layers to cooresponding dicts\n      embeddings_transform_layers.update({embeddings_transform_layer_name : embeddings_transform_layer})\n      embeddings_reshape_layers.update({embeddings_reshape_layer_name : embeddings_reshape_layer})\n      embeddings_bn_layers.update({embeddings_bn_layer_name : embeddings_bn_layer})\n      \n      features_trans_layers.update({features_trans_layer_name:features_trans_layer})\n      features_trans_bn_layers.update({features_trans_bn_layer_name:features_trans_bn_layer})\n    \n    \n    # Define the inputs of the model with a shape (Tx,)\n    # Define s0 (initial hidden state) and c0 (initial cell state)\n    # for the decoder LSTM with shape (n_s,)\n    X = Input(shape=(Tx, input_size))\n    s0 = Input(shape=(n_s,), name='s0')\n    c0 = Input(shape=(n_s,), name='c0')\n    s = s0\n    c = c0\n    \n    # Initialize empty list of outputs\n    outputs = []\n \n    cat_embedding_features_list = []\n    one_hot_features_list = []\n    \n    #generate entity embeddings for each categorical variable in a for loop \n    #i = 0\n    for key in index_positions_dict.keys():\n\n      i  = index_positions_dict[key]\n      index_feature = X[:,:,i]\n\n      index_feature =  tf.cast(index_feature, tf.int32)\n    \n      index_feature_int =  tf.cast(index_feature, tf.int32)\n      index_feauture_one_hot = tf.one_hot(index_feature_int, index_original_dimensions)\n\n      index_feature = embeddings_transform_layers[key + \"embedding_transform\"](index_feature)\n      index_feature = embeddings_reshape_layers[key + \"embedding_reshape\"](index_feature)\n      index_feature = embeddings_bn_layers[key + \"embedding_bn\"](index_feature)\n      \n      index_feature = features_trans_layers[key + \"features_trans\"](index_feature)\n      index_feature = features_trans_bn_layers[key + \"features_trans_bn\"](index_feature)\n      \n      \n      cat_embedding_features_list.append(index_feature)\n      one_hot_features_list.append(index_feauture_one_hot)\n\n    # concatenate the entity embeddings with the numeric features \n    numeric_features = X[:,:,max(len(index_positions_dict.keys())-1,cat_features_amount):]\n    features_list = cat_embedding_features_list\n\n    features_list.append(numeric_features)                    \n    features = tf.keras.layers.concatenate(features_list)\n    cat_embedding_features = tf.keras.layers.concatenate(cat_embedding_features_list)\n    cat_one_hot_features = tf.keras.layers.concatenate(one_hot_features_list)\n    \n    \n    a = Bidirectional(LSTM(units=n_a, return_sequences=True))(numeric_features)\n    \n    # Step 2: Iterate for Ty steps\n    for t in range(Ty):\n        \n        context = one_step_attention(a, s, repeator = repeator, concatenator = concatenator,densor1 = densor1, densor2 = densor2,activator = activator, dotor = dotor )\n        \n        s, _, c = post_activation_LSTM_cell(inputs=context, initial_state=[s, c])\n        \n        out = output_layer_1(inputs=s)\n        out = bn(inputs=out)\n        out = output_layer_2(inputs=out)\n        #out = self.drop(inputs=out)\n        #out = self.output_layer_3(inputs=out)\n        #out = self.output_layer_4(inputs=out)\n        final_out = output_layer_5(inputs=out)\n        \n        # Step 2.D: Append \"out\" to the \"outputs\" list\n        outputs.append(final_out)\n    \n    # Step 3: Create model instance taking three inputs and returning the list of outputs. \n    model = Model(inputs=[X,s0,c0], outputs=outputs)\n       \n    return model \n  \n  \ndef sequence_transcation_fraud_detection(Tx, Ty, n_a, n_s, input_size, out_size):\n    \"\"\"\n    Arguments:\n    Tx -- length of the input sequence\n    Ty -- length of the output sequence\n    n_a -- hidden state size of the Bi-LSTM\n    n_s -- hidden state size of the post-attention LSTM\n\n    Returns:\n    model -- Keras model instance\n    \"\"\"\n    \n    repeator = RepeatVector(Tx)\n    concatenator = Concatenate(axis=-1)\n    densor1 = Dense(10, activation = \"tanh\")\n    densor2 = Dense(1, activation = \"relu\")\n    activator = Activation(\"softmax\", name='attention_weights') \n    dotor = Dot(axes = 1)\n\n    \n    post_activation_LSTM_cell = LSTM(n_s, return_state = True) # post-attention LSTM \n    output_layer_1 = Dense(16, activation=\"relu\")\n    output_layer_2 = Dense(16, activation=\"relu\")\n    output_layer_3 = Dense(16, activation=\"relu\")\n    output_layer_4 = Dense(8, activation=\"relu\")\n    output_layer_5 = Dense(1, activation=\"sigmoid\")\n    bn = keras.layers.BatchNormalization()\n    drop = keras.layers.Dropout(0.5)\n    \n    # Define the inputs of the model with a shape (Tx,)\n    # Define s0 (initial hidden state) and c0 (initial cell state)\n    # for the decoder LSTM with shape (n_s,)\n    X = Input(shape=(Tx, input_size))\n    s0 = Input(shape=(n_s,), name='s0')\n    c0 = Input(shape=(n_s,), name='c0')\n    s = s0\n    c = c0\n    \n    # Initialize empty list of outputs\n    outputs = []\n   \n    a = Bidirectional(LSTM(units=n_a, return_sequences=True))(X)\n    \n    # Step 2: Iterate for Ty steps\n    for t in range(Ty):\n        \n        context = one_step_attention(a, s, repeator = repeator, concatenator = concatenator,densor1 = densor1, densor2 = densor2,activator = activator, dotor = dotor )\n        \n        s, _, c = post_activation_LSTM_cell(inputs=context, initial_state=[s, c])\n        \n        out = output_layer_1(inputs=s)\n        out = bn(inputs=out)\n        out = output_layer_2(inputs=out)\n        #out = self.drop(inputs=out)\n        #out = self.output_layer_3(inputs=out)\n        #out = self.output_layer_4(inputs=out)\n        final_out = output_layer_5(inputs=out)\n        \n        # Step 2.D: Append \"out\" to the \"outputs\" list\n        outputs.append(final_out)\n    \n    # Step 3: Create model instance taking three inputs and returning the list of outputs. \n    model = Model(inputs=[X,s0,c0], outputs=outputs)\n       \n    return model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7b45c97d-9c47-4cc6-99f5-9f997f8944f2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Load in the data sets"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8069b0ff-d450-43d4-83f3-e3dc1bfa4aa2"}}},{"cell_type":"code","source":["#04 -- 08\n\nlabel_col  = \"doublelabel\"\ncustomer = \"none\"\nstart_test = \"2019-12-01\"\nend_test = \"2019-12-31\"\nend_train = \"2019-08-31\"\n\ntrain_path = '/tuser/hive/warehouse/xinghao/tensorflow/ohe/seq_ee_viva_cards_train_1904_1908_sample_rate_004.tfrecord'\ntest_path = '/tuser/hive/warehouse/xinghao/tensorflow/ohe/seq_ee_viva_cards_test_1912_full.tfrecord'\n\n\n\ntrain= spark.table(\"xinghao.seq_ee_viva_cards_train_1904_1908_sample_rate_004\")\n\ntest = spark.table(\"xinghao.seq_ee_viva_cards_test_1912_full\")\n\nfeatures_size = 471 \n\ntrain_size = train.count()\ntest_size = test.count()\n\n\ntest_label_table  = test.filter((test.row_number % 2) == 0).select([\"authtimestamp\",\"doublelabel\"])\ntrain_label_table  = train.filter((train.row_number % 2) == 0).select([\"authtimestamp\",\"doublelabel\"])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db9afb26-a83f-4c73-8c43-7f65643585d3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Decode the tf_records for training and testing"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"272a9c8c-56f5-4de9-80aa-2c64cd7b7aa4"}}},{"cell_type":"code","source":["#Dict first only of float32, int64, string\ndef decode(serialized_example, features_dict=None):\n  \"\"\"\n  Parses an image and label from the given `serialized_example`.\n  It is used as a map function for `dataset.map`\n  \"\"\"\n  if features_dict is None:\n    features_dict = {\n          'features': tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True),\n          'doublelabel': tf.io.FixedLenFeature([], tf.float32),\n      }\n  \n # 1. define a parser\n  features = tf.io.parse_single_example(\n      serialized_example,\n      # Defaults are not specified since both keys are required.\n      features=features_dict)\n\n  # 2. Convert the data\n  feats = features['features']\n  label = features['doublelabel']\n  \n  return feats, label\n\n\n\nfilenames = [(\"/dbfs\" + train_path + \"/\" + name) for name in os.listdir(\"/dbfs\" + train_path) if name.startswith(\"part\")]\ntrain_dataset_exp = tf.data.TFRecordDataset(filenames)\ntrain_dataset_exp = train_dataset_exp.map(decode)\n\n\nfilenames = [(\"/dbfs\" + test_path + \"/\" + name) for name in os.listdir(\"/dbfs\" + test_path) if name.startswith(\"part\")]\ntest_dataset_exp = tf.data.TFRecordDataset(filenames)\ntest_dataset_exp = test_dataset_exp.map(decode)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b33a14b0-f4cd-4ca3-b9a3-5c58175f7253"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Extract features and labels from training and testing sets"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b928642-c274-4656-aada-5d9c8523e8ee"}}},{"cell_type":"code","source":["for batch in train_dataset_exp.batch(train_size).take(1):\n  train_features = batch[0]\n  train_labels = batch[1] \n\n  \nfor batch in test_dataset_exp.batch(test_size).take(1):\n  test_features = batch[0]\n  test_labels = batch[1] \n  #numerical = batch[2]\n  #print(autoencoder_model(features,training= False))\n  print(test_features)\n  print(test_labels)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a3ae506-c6d8-43d1-93af-6016e70af6bc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Reshape the features and labels for training and testing"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9efa94f5-c12f-43af-becd-f1de804d16a9"}}},{"cell_type":"code","source":["train_fea = tf.reshape(train_features,[int(train_size/2),2,features_size])\n#valid_fea = tf.reshape(valid_features,[int(validation_size/2),2,features_size])\ntest_fea = tf.reshape(test_features,[int(test_size/2),2,features_size])\n#test_lab = tf.reshape(test_labels,[30267,1,2])\n\ntrain_label = tf.reshape(train_labels,[int(train_size/2),2,1])\nlabel = tf.transpose(train_label, [1, 0, 2])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f40fded-3317-47cf-8091-5a341e508534"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Training"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"67b7dfc1-71bf-4838-b002-723ec4aec7db"}}},{"cell_type":"code","source":["from keras.optimizers import SGD\nopt = SGD(lr=0.01)\n\n#opt = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01)\n#model.compile(loss = \"binary_crossentropy\", optimizer = opt,metrics=[\"accuracy\"])\n#model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\nMETRICS = [\n      keras.metrics.TruePositives(name='tp'),\n      keras.metrics.FalsePositives(name='fp'),\n      keras.metrics.TrueNegatives(name='tn'),\n      keras.metrics.FalseNegatives(name='fn'), \n      keras.metrics.BinaryAccuracy(name='accuracy'),\n      keras.metrics.Precision(name='precision'),\n      keras.metrics.Recall(name='recall'),\n      keras.metrics.AUC(name='auc'),\n]\n\nEPOCHS = 200\nBATCH_SIZE = 2048\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor= \"val_dense_6_1_auc\", \n    verbose=1,\n    patience=20,\n    mode='max',\n    restore_best_weights=True)\n\n\n\nindex_positions_dict = {\"mcccode\":0,\"issuingbank\":1,\"merchant\":2,\"cardcommercial\":3,\"cardcountry\":4}\n\nindex_original_dimensions_dict =  {\"mcccode\":244,\"issuingbank\":2618,\"merchant\":16673,\"cardcommercial\":2,\"cardcountry\":142}\n\nindex_desired_dimensions_dict = {\"mcccode\":32,\"issuingbank\":48,\"merchant\":32,\"cardcommercial\":2,\"cardcountry\":16}\n\nmodel = sequence_transcation_fraud_detection_combined_with_entity_embeddings(2, 2, 64, 64, 471, 1, index_positions_dict, index_original_dimensions_dict,index_desired_dimensions_dict, 5 )\n\nn_a = 64 # number of units for the pre-attention, bi-directional LSTM's hidden state 'a'\nn_s = 64 # number of units for the post-attention LSTM's hidden state \"s\"\n\n  \ns0 = tf.zeros([int(train_size/2), n_s], tf.int32)\nc0 = tf.zeros([int(train_size/2), n_s], tf.int32)\n  \n\nlr = tf.keras.optimizers.schedules.ExponentialDecay(0.001, decay_steps=100, decay_rate=0.9, staircase=False)\noptimizer = tf.keras.optimizers.Adam(lr)\n\n# keras.optimizers.Adam(lr=1e-4)\n\nmodel.compile(\n      optimizer= keras.optimizers.Adam(lr=1e-4),\n      loss=keras.losses.BinaryCrossentropy(),\n      metrics=METRICS)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c16f9553-e214-4ab5-881d-f032ca3f58cb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["tf.keras.backend.clear_session() \nmodel.fit([train_fea,s0, c0], list(label), epochs=200,callbacks = [early_stopping], batch_size=1000, validation_split=0.2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92bd4311-4539-4a5e-9fd9-ee3e9998d7c0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Results"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e41cda0c-5a6e-42fa-a96a-9a9c149c68ac"}}},{"cell_type":"code","source":["firstelement=udf(lambda v:float(v[0]),FloatType())\nsecondelement=udf(lambda v:float(v[1]),FloatType())\n\ns1 = tf.zeros([int(test_size/2), n_s], tf.int32)\nc1 = tf.zeros([int(test_size/2), n_s], tf.int32)\n\n#s1 = np.zeros((int(test_size),n_s))\n#c1 = np.zeros((int(test_size),n_s))\n\ntest_pred = model.predict([test_fea,s1, c1])\ntrain_pred = model.predict([train_fea,s0, c0])\n\ntrain_pred = pd.DataFrame(train_pred[1])\ntest_pred = pd.DataFrame(test_pred[1])\n\ntrain_pred[1] = train_pred[0]\ntrain_pred[0] = 1- train_pred[0]\ntest_pred[1] = test_pred[0]\ntest_pred[0] = 1- test_pred[0]\n\n#test_label = pd.DataFrame(test_label_table2)\ntest_label = test_label_table.toPandas()\n#test_label = test_label.rename(columns={0: \"authtimestamp\",1:\"doublelabel\"})\ntest_label = test_label.rename(columns={\"cb_flag\":\"doublelabel\"})\n\n\n#train_label = pd.DataFrame(train_label_table)\ntrain_label = train_label_table.toPandas()\n#train_label = train_label.rename(columns={0: \"authtimestamp\",1:\"doublelabel\"})\ntrain_label = train_label.rename(columns={\"cb_flag\":\"doublelabel\"})\n\nresult_test = pd.concat([test_label, test_pred], axis=1).reindex(test_pred.index)\nresult_train = pd.concat([train_label, train_pred], axis=1).reindex(train_pred.index)\n\nresult_test = spark.createDataFrame(result_test)\n\nfrom pyspark.sql.functions import when\nresult_test = result_test.withColumn(\"predictions\", \\\n              when((result_test[\"0\"] > 0.8) , 0).otherwise(1))\nresult_test = result_test.withColumn(\"minus_pred\", -1*result_test[\"0\"])\n\nresult_train= spark.createDataFrame(result_train)\n\nfrom pyspark.sql.functions import when\nresult_train = result_train.withColumn(\"predictions\", \\\n              when((result_train[\"0\"] > 0.8) , 0).otherwise(1))\nresult_train = result_train.withColumn(\"minus_pred\", -1*result_train[\"0\"])\n\nresult_test=VectorAssembler(inputCols=[\"0\",\"minus_pred\"], outputCol=\"rawPrediction\").transform(result_test)  \nresult_test=VectorAssembler(inputCols=[\"0\",\"1\"], outputCol=\"probability\").transform(result_test)\n\nresult_train=VectorAssembler(inputCols=[\"0\",\"minus_pred\"], outputCol=\"rawPrediction\").transform(result_train)  \nresult_train=VectorAssembler(inputCols=[\"0\",\"1\"], outputCol=\"probability\").transform(result_train)\n\nresult_test = result_test.drop(\"minus_pred\")\nresult_train = result_train.drop(\"minus_pred\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aca26753-70b2-4fe1-a30e-5883e8257acd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["train_pred=result_train.withColumn(\"prob1\",firstelement(F.col(\"probability\")))\\\n                     .withColumn(\"prob2\",secondelement(F.col(\"probability\")))\\\n                     .withColumn(\"rawpred1\",firstelement(F.col(\"rawprediction\")))\\\n                     .withColumn(\"rawpred2\",secondelement(F.col(\"rawprediction\")))\n\n\ntest_pred=result_test.withColumn(\"prob1\",firstelement(F.col(\"probability\")))\\\n                   .withColumn(\"prob2\",secondelement(F.col(\"probability\")))\\\n                   .withColumn(\"rawpred1\",firstelement(F.col(\"rawprediction\")))\\\n                   .withColumn(\"rawpred2\",secondelement(F.col(\"rawprediction\")))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2daaf0f5-fce5-4d42-8845-dc260d6e52e3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def show_null_count(df):\n  #display([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns])\n  display(df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5abbaa44-f669-4d76-b210-10ecea5bb258"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def getscoreswithrecall(predbythreshold,TPRgoal):\n  predfilter = predbythreshold.where(predbythreshold.recall <=TPRgoal)\n  tpr = predfilter.agg({\"recall\": \"max\"}).head()[0]\n  fpr = predfilter.agg({\"fpr\": \"max\"}).head()[0]\n  threshold = predfilter.agg({\"threshold\": \"min\"}).head()[0]\n  f.write(\"FPR: \"+ str(round(fpr,3))+\", TPR: \"+ str(round(tpr,3))+\", Threshold: \"+str(round(threshold,3))+\"\\n\")\n  return fpr,tpr,threshold"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54a4f72d-3a71-4c66-bedc-0b93edeaab0e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Scores test set\nhandypredictions = test_pred.toHandy()\nbcm = BinaryClassificationMetrics(handypredictions, scoreCol='probability', labelCol=label_col)\npredbythreshold_overall=bcm.getMetricsByThreshold()\nfpr01,tpr01,threshold01 = reusable_functions.getscoreswithfpr(predbythreshold_overall,0.1)\nfpr001,tpr001,threshold001 = reusable_functions.getscoreswithfpr(predbythreshold_overall,0.01)\nfpr0001,tpr0001,threshold0001 = reusable_functions.getscoreswithfpr(predbythreshold_overall,0.001)\n\nif customer != \"none\":\n  #Scores unseen test set\n  handypredictions = unseen_test_pred.toHandy()\n  bcm = BinaryClassificationMetrics(handypredictions, scoreCol='probability', labelCol=label_col)\n  predbythreshold_unseen=bcm.getMetricsByThreshold()\n  unseen_fpr01,unseen_tpr01,unseen_threshold01 = reusable_functions.getscoreswithfpr(predbythreshold_unseen,0.1)\n  unseen_fpr001,unseen_tpr001,unseen_threshold001 = reusable_functions.getscoreswithfpr(predbythreshold_unseen,0.01)\n  unseen_fpr0001,unseen_tpr0001,unseen_threshold0001 = reusable_functions.getscoreswithfpr(predbythreshold_unseen,0.001)\n\n#Scores start week\nstart_week=start_test\nend_week=datetime.strftime(datetime.strptime(start_week, \"%Y-%m-%d\") + timedelta(days=7), \"%Y-%m-%d\")\nweek_pred=handypredictions.filter(col(\"authtimestamp\")>=start_week).filter(col(\"authtimestamp\")<end_week)\npredbythreshold_start = BinaryClassificationMetrics(week_pred, scoreCol='probability', labelCol=label_col).getMetricsByThreshold()\nfpr,tpr01_start,threshold = reusable_functions.getscoreswithfpr(predbythreshold_start,0.1)\nfpr,tpr001_start,threshold = reusable_functions.getscoreswithfpr(predbythreshold_start,0.01)\nfpr,tpr0001_start,threshold = reusable_functions.getscoreswithfpr(predbythreshold_start,0.001)\n\n#Scores end week\nstart_week=datetime.strftime(datetime.strptime(end_test, \"%Y-%m-%d\") - timedelta(days=7), \"%Y-%m-%d\")\nend_week=end_test\nweek_pred=handypredictions.filter(col(\"authtimestamp\")>=start_week).filter(col(\"authtimestamp\")<end_week)\npredbythreshold_end = BinaryClassificationMetrics(week_pred, scoreCol='probability', labelCol=label_col).getMetricsByThreshold()\nfpr,tpr01_end,threshold = reusable_functions.getscoreswithfpr(predbythreshold_end,0.1)\nfpr,tpr001_end,threshold = reusable_functions.getscoreswithfpr(predbythreshold_end,0.01)\nfpr,tpr0001_end,threshold = reusable_functions.getscoreswithfpr(predbythreshold_end,0.001)\n\n#Decrease in score\nif tpr01_start==0:\n  perfdecr_01=0\nelse:\n  perfdecr_01=(tpr01_end-tpr01_start)/tpr01_start\n\nif tpr001_start==0:\n  perfdecr_001=0\nelse:\n  perfdecr_001=(tpr001_end-tpr001_start)/tpr001_start\n\nif tpr0001_start==0:\n  perfdecr_0001=0\nelse:\n  perfdecr_0001=(tpr0001_end-tpr0001_start)/tpr0001_start"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54d1581a-bea4-4a5e-9805-c655c85b6d90"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["bcm.plot_roc_curve()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b502538-450f-4647-a798-f1daf3001663"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import math\nfrom multiprocessing.pool import ThreadPool\n\ndef mcc_score_per_threshold(predictions, label_col, threshold, verbose=False):\n  predictions= predictions.withColumn(\"prediction\",F.when(col(\"prob2\")>threshold,1.0).otherwise(0.0))\n#   print(predictions.head())\n  tp= predictions.filter((col(label_col) == 1) & (col(\"prediction\") == 1)).count()\n  tn= predictions.filter((col(label_col) == 0) & (col(\"prediction\") == 0)).count()\n  fp= predictions.filter((col(label_col) == 0) & (col(\"prediction\") == 1)).count()\n  fn= predictions.filter((col(label_col) == 1) & (col(\"prediction\") == 0)).count()\n  \n  try:\n    mcc = (tp * tn - fp * fn) / math.sqrt((tp + fp) * (tp + fn) * (fp + tn) * (tn + fn))\n    recall = tp/(tp+fn)\n    precision = tp/(tp+fp)\n  except:\n    print(\"Exception raised for threshold {}. Setting mcc score to 0. Partial results: True Positives {}, True Negatives {}, False Positives {}, False Negatives {}\".format(threshold, tp, tn, fp, fn))\n    mcc = 0\n    recall = 0\n    precision = 0\n  \n  if verbose:\n      print(\"Threshold: {}, Mcc: {}, Recall: {}. True Positives {}, True Negatives {}, False Positives {}, False Negatives {}\".format(threshold, mcc, recall, tp, tn, fp, fn))\n  return mcc, threshold, recall, precision\n\ndef max_mcc_score(predictions, label_col, parallel=False, min_recall=0.25):\n  \n  import numpy as np\n  listOfThresholds = np.arange(0, 1, 0.02).tolist()\n  \n  if parallel:\n    pool = ThreadPool(10)\n    scores = pool.map(lambda threshold: mcc_score_per_threshold(predictions, label_col, threshold), listOfThresholds)\n  else:\n    scores = [mcc_score_per_threshold(predictions, label_col, threshold) for threshold in listOfThresholds]\n    \n  # get mcc scores with recall > min_recall\n  scores_min_recall = [mcc if recall>min_recall else 0 for mcc, threshold, recall, precision in scores]\n  max_mcc_idx=np.argmax(scores_min_recall)\n  return scores[max_mcc_idx] # mcc, threshold, recall, precision"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a00926c5-3e8f-4d6f-bccc-b1a76655a7e4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#max_mcc_tuple=reusable_functions.max_mcc_score(test_pred, label_col, True, 0.25) # mcc, threshold, recall, precision\n\nmax_mcc_tuple=max_mcc_score(test_pred, label_col, True, 0.25)\n\ndef print_cm_by_thresh(predictions,threshold, print_cm = True):\n  predictions= predictions.withColumn(\"prediction\",F.when(col(\"prob2\")>threshold,1.0).otherwise(0.0))\n  tp= predictions.filter((col(label_col) == 1) & (col(\"prediction\") == 1)).count()\n  tn= predictions.filter((col(label_col) == 0) & (col(\"prediction\") == 0)).count()\n  fp= predictions.filter((col(label_col) == 0) & (col(\"prediction\") == 1)).count()\n  fn= predictions.filter((col(label_col) == 1) & (col(\"prediction\") == 0)).count()\n  if print_cm:\n    print(\"TP: \", tp)\n    print(\"FP: \", fp)\n    print(\"TN: \", tn)\n    print(\"FN: \", fn)\n  return [tp,tn,fp,fn]\n\nmax_mcc_tp, max_mcc_tn, max_mcc_fp, max_mcc_fn = print_cm_by_thresh(test_pred, max_mcc_tuple[1], print_cm = False)\n\nprecision_sum = max_mcc_tp + max_mcc_fp\nrecall_sum = max_mcc_tp + max_mcc_fn\nif (max_mcc_tp == 0) or (precision_sum == 0) or (recall_sum == 0):\n  max_mcc_precision = 0\n  max_mcc_recall = 0\nelse:\n  max_mcc_precision = max_mcc_tp / (precision_sum)\n  max_mcc_recall = max_mcc_tp / (recall_sum)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf203dd4-4319-4d88-bb39-559142d1aa25"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def green_score(predictions,threshold):\n  predictions = predictions.withColumn(\"prediction\",F.when(col(\"prob2\")>threshold,1.0).otherwise(0.0))\n  tp= predictions.filter((col(label_col) == 1) & (col(\"prediction\") == 1)).count()\n  tn= predictions.filter((col(label_col) == 0) & (col(\"prediction\") == 0)).count()\n  fp= predictions.filter((col(label_col) == 0) & (col(\"prediction\") == 1)).count()\n  fn= predictions.filter((col(label_col) == 1) & (col(\"prediction\") == 0)).count()\n  try:\n    tnr = tn / (tn+fp)\n    recall = tp / (tp+fn)\n    avg_green = (tnr + recall) / 2\n    return [avg_green, tnr, recall]\n  except:\n    return 0\n  \nlistOfThresholds = np.arange(0.01,0.2,0.04) #np.arange(0,0.2,0.01) #[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\npool = ThreadPool(5)\n\ngreen_results = pool.map(lambda threshold: green_score(test_pred,threshold), listOfThresholds)\nfor i, thresh in enumerate(listOfThresholds):\n  green_results[i].append(thresh)\n  green_info = max(green_results)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"869ca4b8-c1d2-4213-8784-3d3a19fde164"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Plot ROC-Curve\nclass CurveMetrics(BinaryClassificationMetrics):\n    def __init__(self, *args):\n        super(CurveMetrics, self).__init__(*args)\n\n    def _to_list(self, rdd):\n        points = []\n        for row in rdd.collect():\n            points += [(float(row._1()), float(row._2()))]\n        return points\n\n    def get_curve(self, method):\n        rdd = getattr(self._java_model, method)().toJavaRDD()\n        return self._to_list(rdd)\n\n# Returns as a list (false positive rate, true positive rate)\npreds = test_pred.select(label_col,'probability').rdd.map(lambda row: (float(row['probability'][1]), float(row[label_col])))\npoints = CurveMetrics(preds).get_curve('roc')\n\nx_val = [x[0] for x in points]\ny_val = [x[1] for x in points]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3bffbca5-afdf-448f-80ac-a89993e3b4b1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def find_nearest(array, value):\n    array = np.asarray(array)\n    idx = (np.abs(array - value)).argmin()\n    return idx,array[idx]\n  \ndef get_x_for_y_points(x_list, y_list, y_points = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]):\n  x_y_list = []\n  for y in y_points:\n    y_idx, value = find_nearest(y_list, y)\n    x = round(x_list[y_idx],5)\n    x_y_list.append( (y,x) ) \n    \n  return x_y_list\n\n#Compute FPR at interval of TPR\nrecall_fpr_list = get_x_for_y_points(x_val,y_val)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7beb548c-2d2a-4501-832c-8767a2a0cf71"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["evaluator = BinaryClassificationEvaluator()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f96f32b8-c6fd-4468-929a-cfbe318ec9f1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["mlflow.end_run()\nmlflow.start_run()\n  \nmlflow.log_param(\"training period\", \"2019-06-01 to 2019-06-30\")\nmlflow.log_param(\"testing period\", \"2019-10-01 to 2020-10-31\")\n\nmlflow.log_param(\"num_features\", features_size)\n\nmlflow.log_param(\"hidden units of LSTM layers\", 64)\nmlflow.log_param(\"output_dim\", 2)\nmlflow.log_param(\"group col\", \"card_approx\")\n\nmlflow.log_param(\"num_time_steps\", 2)\nmlflow.log_param(\"units of output_layer_1\", 16)\nmlflow.log_param(\"activation of output_layer1\", \"relu\")\nmlflow.log_param(\"units of output_layer_2\", 8)\nmlflow.log_param(\"activation of output_layer2\", \"relu\")\nmlflow.log_param(\"units of output_layer_3\", 1)\nmlflow.log_param(\"activation of output_layer3\", \"sigmoid\")\n\nmlflow.log_param(\"norm_type\", \"batch\")\nmlflow.log_param(\"tunning\", \"early_stoping\")\nmlflow.log_param(\"monitor\", \"valid_auc\")\nmlflow.log_param(\"batch_size\", 1000)\nmlflow.log_param(\"EPOCHS\", 200)\ntrain_scorelabel=train_pred.select(col(label_col).alias(\"label\"),\"rawPrediction\")\ntest_scorelabel=test_pred.select(col(label_col).alias(\"label\"),\"rawPrediction\")\n\nmlflow.log_metric(\"trainAreaUnderROC\", evaluator.evaluate(train_scorelabel, {evaluator.metricName: \"areaUnderROC\"}), step=1)\nmlflow.log_metric(\"trainAreaUnderPR\", evaluator.evaluate(train_scorelabel, {evaluator.metricName: \"areaUnderPR\"}), step=1)\nmlflow.log_metric(\"testAreaUnderROC\", evaluator.evaluate(test_scorelabel, {evaluator.metricName: \"areaUnderROC\"}), step=1)\nmlflow.log_metric(\"testAreaUnderPR\", evaluator.evaluate(test_scorelabel, {evaluator.metricName: \"areaUnderPR\"}), step=1)\nif customer != \"none\":\n  mlflow.log_metric(\"unseen_TPR_forFPR01\", unseen_tpr01, step=1)\n  mlflow.log_metric(\"unseen_TPR_forFPR001\", unseen_tpr001, step=1)\n  mlflow.log_metric(\"unseen_TPR_forFPR0001\", unseen_tpr0001, step=1)\n\nmlflow.log_metric(\"TPR_forFPR01\", tpr01, step=1)\nmlflow.log_metric(\"TPR_forFPR001\", tpr001, step=1)\nmlflow.log_metric(\"TPR_forFPR0001\", tpr0001, step=1)\nmlflow.log_metric(\"TPRdecr_forFPR01\", perfdecr_01, step=1)\nmlflow.log_metric(\"TPRdecr_forFPR001\", perfdecr_001, step=1)\nmlflow.log_metric(\"TPRdecr_forFPR0001\", perfdecr_0001, step=1)  \n\nmlflow.log_metric(\"MCC\", max_mcc_tuple[0], step=1)\nmlflow.log_metric(\"MCC threshold\", max_mcc_tuple[1], step=1)\nmlflow.log_metric(\"MCC threshold precision\", max_mcc_precision , step=1)\nmlflow.log_metric(\"MCC threshold recall\", max_mcc_recall, step=1)\n\nmlflow.log_metric(\"MCC TP\", max_mcc_tp, step=1)\nmlflow.log_metric(\"MCC FP\", max_mcc_fp, step=1)\nmlflow.log_metric(\"MCC TN\", max_mcc_tn, step=1)\nmlflow.log_metric(\"MCC FN\", max_mcc_fn, step=1)\n\nmlflow.log_metric(\"Recall 10 pct\", recall_fpr_list[0][1], step=1)\nmlflow.log_metric(\"Recall 20 pct\", recall_fpr_list[1][1], step=1)\nmlflow.log_metric(\"Recall 30 pct\", recall_fpr_list[2][1], step=1)\nmlflow.log_metric(\"Recall 40 pct\", recall_fpr_list[3][1], step=1)\nmlflow.log_metric(\"Recall 50 pct\", recall_fpr_list[4][1], step=1)\nmlflow.log_metric(\"Recall 60 pct\", recall_fpr_list[5][1], step=1)\nmlflow.log_metric(\"Recall 70 pct\", recall_fpr_list[6][1], step=1)\nmlflow.log_metric(\"Recall 80 pct\", recall_fpr_list[7][1], step=1)\nmlflow.log_metric(\"Recall 90 pct\", recall_fpr_list[8][1], step=1)\n\nmlflow.log_metric(\"Green TNR \", green_info[1] , step=1) #High TNR is higher TN\nmlflow.log_metric(\"Green recall \", green_info[2], step=1) #Higher TPR means Lower FN\nmlflow.log_metric(\"Green measure avg TNR and recall\", green_info[0], step=1)\nmlflow.log_metric(\"Green measure threshold\", green_info[3], step=1)\n\nstart_week=start_test\nend_week=datetime.strftime(datetime.strptime(start_week, \"%Y-%m-%d\") + timedelta(days=7), \"%Y-%m-%d\")\nwith open(\"performance_per_week_exp.txt\", \"w\") as f:\n  f.write(\"Train set ends at \"+end_train+\"\\n\")\n  while end_week <= end_test:\n    week_pred=handypredictions.filter(col(\"authtimestamp\")>=start_week).filter(col(\"authtimestamp\")<end_week)\n    bcm = BinaryClassificationMetrics(week_pred, scoreCol='probability', labelCol=label_col)\n    predbythreshold_week=bcm.getMetricsByThreshold()\n    fpr01,tpr01,threshold01 = reusable_functions.getscoreswithfpr(predbythreshold_week,0.1)\n    fpr001,tpr001,threshold001 = reusable_functions.getscoreswithfpr(predbythreshold_week,0.01)\n    fpr0001,tpr0001,threshold0001 = reusable_functions.getscoreswithfpr(predbythreshold_week,0.001)\n    f.write(start_week+\" - \"+end_week+\", 10%: \"+str(round(tpr01,3))+\", 1%: \"+ str(round(tpr001,3))+\", 0.1%: \"+str(round(tpr0001,3))+\"\\n\")\n    start_week=datetime.strftime(datetime.strptime(start_week, \"%Y-%m-%d\") + timedelta(days=7), \"%Y-%m-%d\")\n    end_week=datetime.strftime(datetime.strptime(end_week, \"%Y-%m-%d\") + timedelta(days=7), \"%Y-%m-%d\")\nmlflow.log_artifact(\"performance_per_week_exp.txt\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2868306c-bd3f-4a0f-90da-8a052246302e"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"3_modeling","dashboards":[{"elements":[],"guid":"a4025321-b882-4c2e-aa5c-2688d9a6c3a0","layoutOption":{"stack":true,"grid":true},"version":"DashboardViewV1","nuid":"45950d71-4466-4e92-8765-ac1aaf07728a","origId":592403,"title":"Untitled","width":1024,"globalVars":{}},{"elements":[{"elementNUID":"8069b0ff-d450-43d4-83f3-e3dc1bfa4aa2","guid":"52dd1398-73ab-4035-a78b-3c3ba37739ab","options":null,"position":{"x":0,"y":2,"height":2,"width":12,"z":null},"elementType":"command"}],"guid":"7611dab1-c30a-40bd-9e65-65f441ec8f1a","layoutOption":{"stack":true,"grid":true},"version":"DashboardViewV1","nuid":"b0f2017a-82e1-4d0b-8770-2a3bd93efabf","origId":592404,"title":"Untitled","width":1024,"globalVars":{}}],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":592370}},"nbformat":4,"nbformat_minor":0}
